{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### import things\n",
    "import pandas as pd\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "### read form the csv doc\n",
    "data_train = pd.read_csv('train.csv')  \n",
    "data_test = pd.read_csv('test.csv')\n",
    "### trans to array\n",
    "train_array = np.array(data_train)\n",
    "test_array = np.array(data_test)\n",
    "###############hyper parameter\n",
    "use_normA=0\n",
    "use_normy=0\n",
    "###############hyper parameter\n",
    "\n",
    "###############functions\n",
    "\n",
    "### determin the x_tilda\n",
    "def cp(x):\n",
    "    return copy.deepcopy(x)\n",
    "\n",
    "### get w_tilda\n",
    "def compute_w_tilda(iters_inner,w_prev,ww_prev,all_or_one='mix',alpha=0.8,beta=0.2):\n",
    "    if all_or_one=='all':\n",
    "        w_tilda=cp(ww_prev)/iters_inner\n",
    "    elif all_or_one=='one':\n",
    "        w_tilda=cp(w_prev)\n",
    "    elif all_or_one=='mix':\n",
    "        w_tilda= cp(alpha*(w_prev)+beta*(ww_prev/iters_inner))\n",
    "    return w_tilda\n",
    "\n",
    "### get the data matrix A\n",
    "def get_A(use_normA,train_array):\n",
    "    if use_normA==1: \n",
    "        A_before_norm = train_array\n",
    "        A=np.empty(np.shape(A_before_norm))\n",
    "        for i in range(np.shape(A_before_norm)[1]):\n",
    "            temp=A_before_norm[:,i]/np.linalg.norm(A_before_norm[:,i])\n",
    "            A[:,i]=temp\n",
    "    else:\n",
    "        A = train_array\n",
    "    return A\n",
    "\n",
    "### get alpha \n",
    "def compute_alpha(A,lamb):\n",
    "    L_max=0.25*max(sum(A*A))\n",
    "    alpha=0.15/(L_max+lamb)\n",
    "    return alpha\n",
    "\n",
    "def get_xx_0():\n",
    "    xx_0=np.zeros([85*67,67])\n",
    "    for i in range(67):\n",
    "        xx_0[i*85:i*85+85][:,i]=1\n",
    "    return xx_0\n",
    "\n",
    "def compute_res(y,x_phi,lamb):\n",
    "    return sum((y-A.dot(x_phi))*(y-A.dot(x_phi)))\n",
    "    \n",
    "def get_index_min(xx_0,w_tilda):\n",
    "    res_min=1e9\n",
    "    for m in range(np.shape(xx_0)[1]):\n",
    "        temp=xx_0[:,m].reshape(5695,1)\n",
    "        x_phi=w_tilda*temp\n",
    "        res=compute_res(y,x_phi,lamb)\n",
    "        if res<res_min:\n",
    "            res_min=res\n",
    "            index_min=m\n",
    "            print('min_index_now',index_min)\n",
    "    return index_min\n",
    "\n",
    "def compute_sci(xx_0,w):\n",
    "    w_l1=np.linalg.norm(w,ord=1)\n",
    "    temp_max=1e-9\n",
    "    for m in range(np.shape(xx_0)[1]):\n",
    "        temp=xx_0[:,m].reshape(5695,1)\n",
    "        x_phi=w_tilda*temp\n",
    "        x_phi_l1=np.linalg.norm(x_phi,ord=1)\n",
    "        if x_phi_l1>temp_max:\n",
    "            temp_max=x_phi_l1\n",
    "    return (67*(temp_max/w_l1)-1)/(67-1)\n",
    "\n",
    "\n",
    "\n",
    "###############functions\n",
    "\n",
    "\n",
    "### process the A to be norm or not\n",
    "A=get_A(0,train_array)\n",
    "[d,n] = A.shape\n",
    "L_max=0.25*max(sum(A*A))\n",
    "\n",
    "### aha! same as the lambda, the L-1 norm coefficent\n",
    "# lamb=1e-5\n",
    "lamb=1e-4\n",
    "\n",
    "### the learning rate 0.15/(the proxi-L constant)\n",
    "alpha=compute_alpha(A,lamb)\n",
    "\n",
    "### loops\n",
    "iters_outer=20\n",
    "iters_inner=1*d\n",
    "\n",
    "### count the number bingo\n",
    "right=0\n",
    "\n",
    "xx_0=get_xx_0()\n",
    "\n",
    "rand_index=2383\n",
    "#     print('rand_index',rand_index)\n",
    "rand_who=int(rand_index/85)\n",
    "#     print('rand_who',rand_who)\n",
    "\n",
    "#### which y version you use, norm processed or the plain one \n",
    "if use_normy==1:\n",
    "    temp_y = test_array[:,rand_index].reshape(d,1)\n",
    "    y=np.empty(np.shape(temp_y))\n",
    "    y=temp_y/np.linalg.norm(temp_y)\n",
    "else:\n",
    "    y=test_array[:,rand_index].reshape(d,1)\n",
    "\n",
    "### initialize the two ws\n",
    "w_prev=np.array(np.zeros([n,1]))\n",
    "w_tilda=np.array(np.zeros([n,1]))\n",
    "ww_prev=np.array(np.zeros([n,1]))\n",
    "\n",
    "### w_prev hotmap\n",
    "# w_prev_hot=np.empty([n,1])\n",
    "hot_num=0\n",
    "\n",
    "### some hyper params\n",
    "\n",
    "\n",
    "### store the res and time\n",
    "res_list=[]\n",
    "time_list=[]\n",
    "sci_list=[]\n",
    "### count the time \n",
    "start = time.clock()\n",
    "for i in range(iters_outer):\n",
    "    print(i)\n",
    "    ### full grad\n",
    "    mu=(-A.T).dot(y-A.dot(w_tilda))/d\n",
    "    ### hyper params\n",
    "    theta=2/(i+4)\n",
    "    ita=1/(4*L_max*theta)\n",
    "    ##small loop\n",
    "    for j in range(iters_inner):\n",
    "        ### choose a rand one from d \n",
    "        idx=np.random.randint(0,d)\n",
    "        ### A transpose, need reshape\n",
    "        A_idx=A[idx].T.reshape(n,1)\n",
    "        ### compute y as the combine of w_prev and w_tilda\n",
    "        y_w=theta*w_prev+(1-theta)*w_tilda\n",
    "        ### g_prev and tilda\n",
    "#         g_prev=(-A_idx).dot(y[idx]-(A[idx].reshape(1,n)).dot(w_prev)) \n",
    "        ### compute with y_w instead of w_prev\n",
    "        g_prev=(-A_idx).dot(y[idx]-(A[idx].reshape(1,n)).dot(y_w)) \n",
    "        g_tilda=(-A_idx).dot(y[idx]-(A[idx].reshape(1,n)).dot(w_tilda)) \n",
    "        ### update the parameter\n",
    "        w_prev=w_prev-alpha*(g_prev-g_tilda+mu)\n",
    "        ### the soft thresholding\n",
    "        temp=w_prev-lamb*alpha\n",
    "        temp=(temp+abs(temp))/2\n",
    "        w_prev=np.sign(w_prev)*temp\n",
    "        ### sum w_prev\n",
    "        ww_prev+=w_prev\n",
    "\n",
    "        \n",
    "        ### compute the res of w_prev and store\n",
    "#         res_now=sum((y-A.dot(w_prev))*(y-A.dot(w_prev)))\n",
    "#         res_list.append(res_now)\n",
    "#         time_list.append(time.clock()-start)\n",
    "        \n",
    "        ### compute the sci value of w_prev and store\n",
    "#         sci_now=compute_sci(xx_0,w_prev)\n",
    "#         sci_list.append(sci_now)\n",
    "        \n",
    "        ### make the w_prev_hot\n",
    "#         w_prev_hot=np.concatenate((w_prev_hot,w_prev),axis=1)\n",
    "        \n",
    "    ### assign the w_tilda for another loop    \n",
    "#     w_tilda=copy.deepcopy(w_prev)\n",
    "    ### mig combined w_tilda\n",
    "    w_tilda=theta*(ww_prev/iters_inner)+(1-theta)*w_tilda\n",
    "\n",
    "index_min=get_index_min(xx_0,w_tilda)\n",
    "\n",
    "        \n",
    "print(('rand_index',rand_index))\n",
    "print(('randwho and index_min',rand_who,index_min))\n",
    "\n",
    "if rand_who == index_min:\n",
    "    right=right+1\n",
    "    \n",
    "\n",
    "# time_list_name=\"npy/mig\"+'time_list'+'inner'+str(iters_inner)+'outer'+str(iters_outer)+'.npy'\n",
    "# res_list_name=\"npy/mig\"+'res_list'+'inner'+str(iters_inner)+'outer'+str(iters_outer)+'.npy'\n",
    "# sci_list_name=\"npy/mig\"+'sci_list'+'inner'+str(iters_inner)+'outer'+str(iters_outer)+'.npy'\n",
    "# np.save(time_list_name,time_list)\n",
    "# np.save(res_list_name,res_list)\n",
    "# np.save(sci_list_name,sci_list)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "30",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
